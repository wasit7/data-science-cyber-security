{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352b4e1b-d023-4b52-8bc6-52552f515f30",
   "metadata": {},
   "source": [
    "### **Summary of the Code**\n",
    "\n",
    "This script processes Zeek log files, extracts metadata and data, and prepares them for use in a graph-oriented format by converting them into triples (subject-predicate-object). It then saves the results in various formats for further processing.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Steps in the Code**\n",
    "\n",
    "1. **Import Libraries**:\n",
    "   - Utilizes `os`, `gzip`, `pandas`, `re`, and `json` to handle file operations, data processing, and JSON manipulation.\n",
    "\n",
    "2. **Parse Zeek Logs (`parse_zeek_log`)**:\n",
    "   - **Reads the log file**: Opens and decompresses the `.gz` Zeek log file.\n",
    "   - **Extracts metadata**: Parses lines starting with `#` to form a metadata dictionary (e.g., `#fields`, `#types`).\n",
    "   - **Extracts headers**: Identifies the column names for the data rows using the `#fields` line.\n",
    "   - **Extracts data**: Parses data rows (lines that donâ€™t start with `#`) into a pandas DataFrame.\n",
    "   - **Cleans and processes data**:\n",
    "     - Replaces placeholders (`(empty)`, `-`) with `None`.\n",
    "     - Converts column data types based on metadata (e.g., timestamps, intervals, counts).\n",
    "\n",
    "3. **Save Data (`save_to_parquet_and_json`)**:\n",
    "   - **Saves the DataFrame**: Writes the parsed data to a Parquet file for efficient storage.\n",
    "   - **Saves metadata**: Exports metadata to a JSON file.\n",
    "\n",
    "4. **Generate Subject-Predicate-Object (SPO) Triples**:\n",
    "   - Extracts key columns (`id.orig_h`, `id.resp_h`, `proto`, `service`) to form triples:\n",
    "     - `id.orig_h` as `s` (subject).\n",
    "     - `proto:service` as `p` (predicate).\n",
    "     - `id.resp_h` as `o` (object).\n",
    "   - Renames columns (`s`, `p`, `o`) and filters for relevant fields.\n",
    "\n",
    "5. **Save Triples and Metadata**:\n",
    "   - Saves the SPO triples as a CSV file (without headers).\n",
    "   - Saves metadata as a JSON file.\n",
    "\n",
    "6. **Prepare for Knowledge Graph Creation**:\n",
    "   - Includes Jupyter magic commands for using `kg` CLI to add the data and metadata to a graph database.\n",
    "\n",
    "---\n",
    "\n",
    "### **Outputs**\n",
    "\n",
    "1. **Parquet File**:\n",
    "   - The processed Zeek log data is saved as a `.parquet` file for compact storage.\n",
    "2. **JSON Metadata**:\n",
    "   - Extracted metadata is saved in a structured `.json` file.\n",
    "3. **SPO Triples**:\n",
    "   - The triples (subject-predicate-object) are saved as `data.csv` for use in a graph database.\n",
    "\n",
    "---\n",
    "\n",
    "### **Jupyter Commands for Graph Operations**\n",
    "Install:\n",
    "\n",
    "```sh\n",
    "pip install git+https://github.com/wasit7/kgsearch.git\n",
    "```\n",
    "\n",
    "The script includes magic commands for integrating the processed triples into a knowledge graph using a CLI tool (e.g., `kg`):\n",
    "1. **Add Data**:\n",
    "   ```bash\n",
    "   !kg add -f data.csv\n",
    "   ```\n",
    "   Adds the SPO triples from `data.csv` to the graph database.\n",
    "\n",
    "2. **Add Metadata**:\n",
    "   ```bash\n",
    "   !kg meta -f metadata.json\n",
    "   ```\n",
    "   Adds the metadata from `metadata.json` to describe the dataset.\n",
    "\n",
    "3. **Start Knowledge Graph**:\n",
    "   ```bash\n",
    "   !kg start\n",
    "   ```\n",
    "   Initializes and starts the graph database.\n",
    "\n",
    "---\n",
    "\n",
    "### **Enhancements for Usability**\n",
    "1. **Parameterize File Paths**:\n",
    "   Allow dynamic input for file paths to process multiple logs.\n",
    "2. **Error Handling**:\n",
    "   Add error handling for missing files or parsing errors.\n",
    "3. **Additional Metadata**:\n",
    "   Enrich metadata with derived statistics (e.g., row counts, unique IPs).\n",
    "\n",
    "---\n",
    "\n",
    "This script is ready to parse Zeek logs, prepare graph-ready data, and integrate it into a graph database. Let me know if you'd like help extending it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c5cfb7-d9c4-4755-bdcc-b5e968f98fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./_landing/conn_log.parquet\n",
      "Metadata saved to ./_landing/conn_log_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_zeek_log(file_path):\n",
    "    \"\"\"\n",
    "    Parses a Zeek log file into a pandas DataFrame along with metadata.\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {}\n",
    "    for line in lines:\n",
    "        if line.startswith(\"#\"):\n",
    "            parts = line[1:].split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                metadata[parts[0].strip()] = parts[1].strip()\n",
    "    \n",
    "    # Extract headers\n",
    "    headers_line = next(line for line in lines if line.startswith(\"#fields\"))\n",
    "    headers = headers_line.split(\"\\t\")[1:]  # Extract headers after #fields\n",
    "    headers = [header.strip() for header in headers]\n",
    "\n",
    "    # Extract data rows\n",
    "    data_lines = [line.strip().split(\"\\t\") for line in lines if not line.startswith(\"#\")]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_lines, columns=headers)\n",
    "\n",
    "    # Replace Zeek placeholders for missing data\n",
    "    df.replace({'(empty)': None, '-': None}, inplace=True)\n",
    "\n",
    "    # Convert specific types based on metadata if available\n",
    "    if \"#types\" in metadata:\n",
    "        types = metadata[\"#types\"].split(\"\\t\")\n",
    "        for col, dtype in zip(headers, types):\n",
    "            if dtype == \"time\":\n",
    "                df[col] = pd.to_datetime(df[col], unit=\"s\")\n",
    "            elif dtype in {\"interval\", \"count\"}:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            elif dtype == \"bool\":\n",
    "                df[col] = df[col] == \"T\"\n",
    "\n",
    "    return df, metadata\n",
    "\n",
    "def save_to_parquet_and_json(df, metadata, output_dir, base_name):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to Parquet and metadata to JSON.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save DataFrame as Parquet\n",
    "    parquet_file = os.path.join(output_dir, f\"{base_name}.parquet\")\n",
    "    df.to_parquet(parquet_file, index=False)\n",
    "    print(f\"Data saved to {parquet_file}\")\n",
    "\n",
    "    # Save metadata as JSON\n",
    "    metadata_file = os.path.join(output_dir, f\"{base_name}_metadata.json\")\n",
    "    with open(metadata_file, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"Metadata saved to {metadata_file}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"../zeek/logs/2024-12-16/conn.10:00:00-11:00:00.log.gz\"\n",
    "output_dir = \"./_landing\"\n",
    "\n",
    "# Parse the log file\n",
    "df, metadata = parse_zeek_log(file_path)\n",
    "# Save results to Parquet and JSON\n",
    "save_to_parquet_and_json(df, metadata, output_dir, \"conn_log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de82c395-ce23-41a6-868c-f1a37c16b72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>p</th>\n",
       "      <th>o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.168.127.114</td>\n",
       "      <td>udp:dns</td>\n",
       "      <td>192.168.127.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.168.127.114</td>\n",
       "      <td>udp:dns</td>\n",
       "      <td>192.168.127.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192.168.127.114</td>\n",
       "      <td>udp:dns</td>\n",
       "      <td>192.168.127.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.168.127.114</td>\n",
       "      <td>udp:quic</td>\n",
       "      <td>172.217.25.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.168.127.114</td>\n",
       "      <td>udp:dns</td>\n",
       "      <td>192.168.127.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2791</th>\n",
       "      <td>fe80::10c1:6dde:581e:e273</td>\n",
       "      <td>udp:dns</td>\n",
       "      <td>ff02::fb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2792</th>\n",
       "      <td>192.168.28.229</td>\n",
       "      <td>udp:quic</td>\n",
       "      <td>216.58.199.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>192.168.28.229</td>\n",
       "      <td>udp:quic</td>\n",
       "      <td>216.58.200.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>192.168.28.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>255.255.255.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>192.168.28.229</td>\n",
       "      <td>udp:quic</td>\n",
       "      <td>64.233.186.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2796 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              s         p                o\n",
       "0               192.168.127.114   udp:dns  192.168.127.134\n",
       "1               192.168.127.114   udp:dns  192.168.127.134\n",
       "2               192.168.127.114   udp:dns  192.168.127.134\n",
       "3               192.168.127.114  udp:quic   172.217.25.202\n",
       "4               192.168.127.114   udp:dns  192.168.127.134\n",
       "...                         ...       ...              ...\n",
       "2791  fe80::10c1:6dde:581e:e273   udp:dns         ff02::fb\n",
       "2792             192.168.28.229  udp:quic   216.58.199.227\n",
       "2793             192.168.28.229  udp:quic     216.58.200.3\n",
       "2794              192.168.28.11       NaN  255.255.255.255\n",
       "2795             192.168.28.229  udp:quic    64.233.186.94\n",
       "\n",
       "[2796 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spo = df[['id.orig_h','id.resp_h','proto','service']].copy()\n",
    "df_spo['edge'] = (df_spo['proto'] + ':' + df_spo['service']).fillna('')\n",
    "df_spo['edge']= df['proto']+':'+ df['service']\n",
    "df_spo=df_spo.rename(columns={'id.orig_h': 's', 'edge': 'p', 'id.resp_h':'o'})\n",
    "df_spo=df_spo[['s','p','o']]\n",
    "df_spo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de3fa5e-ed57-4a6f-8613-afa4a4588e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file= 'data.csv'\n",
    "# df_spo.to_csv(csv_file, header=False, index=False)\n",
    "metadata_file='metadata.json'\n",
    "\n",
    "df_spo.to_csv(csv_file, header=False, index=False)\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae3d32-7113-40ef-88c3-61edece006c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Starting the app.\n",
      " * Serving Flask app 'kgsearch.app.app' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [17/Dec/2024 14:32:30] \"GET /search/1/1/1/192.168.127.114 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Dec/2024 14:32:33] \"GET /search/1/1/1/192.168.127.114; HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Dec/2024 14:32:35] \"GET /search/1/1/1/192.168.127.114; HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "!kg add -f data.csv\n",
    "!kg meta -f metadata.json\n",
    "!kg start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23890c1-32f9-4356-9e23-0faf2f12205f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
